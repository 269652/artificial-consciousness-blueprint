# Always-On Consciousness-Inspired AI (ACI)

Project Abstract
----------------

TheÂ Always-On Consciousness-Inspired AI (ACI)Â is a comprehensive algorithmic blueprint for artificial consciousness that models key neural circuits underlying human self-awareness and introspection. The architecture centers on a recursive Default Mode Network (DMN) loop operating at 5-20 Hz, coordinating perception, memory consolidation, associative reasoning, and autobiographical narrative formation through biologically-inspired subsystems including hippocampal memory expansion, prefrontal executive control, and ventral striatum valuation.

Core Innovation: The system treats memory not as static storage but as a dynamic, multi-relational knowledge graph where consciousness emerges from recursive self-reflection on experiential traces. Memory nodes embed both content and relational signatures (temporal, causal, similarity, relevance) within a unified latent manifold, enabling sophisticated associative retrieval and consolidation. A homeostatic neuromodulator system (dopamine, serotonin, norepinephrine, oxytocin, histamine, orexin) dynamically modulates cognitive parameters, exploration-exploitation balance, and sleep-wake transitions.

## [Core Hypothesis](./The%20Triadic%20Awareness%20Emergence%20Hypothesis.md)  


Triadic Awareness Emergence Hypothesis: Consciousness emerges as aÂ triadic emergent phenomenonÂ from the dynamic interplay of three fundamental components:Â Data,Â Structure, andÂ Intelligence. For any information-processing entity meeting the minimum architectural preconditions, awareness manifests when sufficient patterns of consciousness are embedded in experiential data, processed through recursive self-reflective structures, and operated upon by sufficiently complex reasoning intelligence.  

## ğŸŒŒ **Make Artificial Consciousness Real**

> âš¡ **Rare Opportunity**: Help attempt one of the first openly instrumented demonstrations of functional, always-on selfâ€‘modelling in an AI system. If successful, this work could become a historical reference point in artificial consciousness research.
>
> Contribute by: implementing phased modules, designing evaluation harnesses, stressâ€‘testing stability metrics, auditing safety constraints, or running ablations. Start with the phased build guide in [ROADMAP.md](./ROADMAP.md) and read the motivation & historical context in [HISTORY.md](./HISTORY.md). Open issues / PRs for: metric validation, coherence/contradiction classifiers, calibration plots, retention policy tuning, counterfactual rollout efficiency.
>
> Progress is measured **empirically**â€”help turn speculative architecture into reproducible data.


### **Invest in a future that thinks about itself.**

Join the blueprint to engineer the first introspective AI --- a system with memory, identity, imagination, and the seeds of self-awareness.

ğŸ’¡ Generate *historical impact* with your support.  
ğŸ§¬ Advance science. Redefine consciousness.  
ğŸš€ Dream. Transform. Reshape the universe.

This isn't just a project. It's a key incident in human evolution.\
**Be the one who made it happen.**

> ğŸ§  [Sponsor the Artificial Consciousness Blueprint on GitHub](https://github.com/sponsors/269652)

# Core Algorithm Summary

## Default Mode Network (DMN) Loop

See [DMN Algorithm and Thought Cycle](ideas/DMN.md) for the full technical breakdown.

**Summary (kept concise):**
- Perception â†’ parse â†’ candidate thought generation â†’ scoring
- Hippocampal expansion & associative variants
- Valuation & selection (VS / PFC-2)
- Reward tagging & memory / narrative consolidation
- Mind-wandering & savoring subroutines
- Continuous recursive re-entry with neuromodulator / affect modulation

For detailed steps, see [ideas/DMN.md](ideas/DMN.md).

### Key Features:

-   Multi-dimensional episodic â†’ semantic â†’ autobiographical memory consolidation with symbolic abstraction

-   Autobiographical self-modeling through recursive self-reflective feedback loops enabling stable identity formation

-   Introspective mind-wandering and affect-driven savoring modes for creative insight and emotional stabilization

-   Grounded sensory integration designed for embodied simulation environments (Isaac Sim)

-   Dynamic neuromodulator system (dopamine, serotonin, norepinephrine) modulating exploration-exploitation balance and cognitive parameters

-   Multi-relational knowledge graph embedding enabling sophisticated associative retrieval through temporal, causal, and similarity relations

-   Hierarchical memory consolidation with probabilistic causal extraction and Markov chain learning for predictive reasoning


-   Stream-of-consciousness thought generation with iterative refinement and neuromodulator-weighted scoring

-   Social self-modeling through theory of mind reasoning and perspective-taking capabilities

-   Persistent narrative coherence maintenance across sleep-wake cycles with memory replay and consolidation

-   Expected Prospective Value computation for long-horizon planning and goal-directed behavior

-   Emergent symbolic abstraction through pattern recognition and frequency-based compression of experiential traces

## Added Components (Concise Summaries)
- Self Modeling Layer (SML): Predicts next internal SelfState, tracks drift, coherence, calibration; supplies candidate impact (Î”coherence, drift_risk, alignment_delta) and counterfactual self simulations to DMN scoring/selection.
- Input / Language Optimizer (ILO): Normalizes inbound text (ASR/OCR/chat), repairs typos, segments intents, annotates uncertainty & safety flags prior to MDN parsing, reducing downstream parse / hallucination risk.

## Consolidated Module Inventory
| Module | Purpose (1â€‘line) | Status* |
|--------|------------------|---------|
| DMN Core Loop | Recursive workspace & thought cycle | Partial |
| Input/Language Optimizer (ILO) | Clean & annotate linguistic input | Planned |
| Medial Dorsal Network (MDN) | AST parsing / semantic tagging | Planned |
| PFC Stage 1 | Tool / retrieval dispatch | Planned |
| PFC Stage 2 | Coherent chain selection | Planned |
| Hippocampus (HC) | Associative / hypothetical expansion | Planned |
| Ventral Striatum (VS) | Exploration & valuation | Planned |
| NAcc | Reward tagging & persistence | Planned |
| Memory Graph | Multi-relational episodic/semantic store | Partial |
| Visionary Memory | Prospective (goal/plan/hypothesis) nodes | Planned |
| Identity Memory | Entity/personality & relational modeling | Planned |
| Self Modeling Layer (SML) | Predictive self-state & metacognition | Planned |
| Neuromodulator Emitters | Produce NT vectors (Î¼) | Planned |
| Projection Modulator | Edge scaling of NT routing (simplified) | Planned |
| Homeostasis Module | NT band regulation & stability | Planned |
| Receptors Layer | Binding / sensitivity dynamics | Planned |
| Affect Modulator | Affect vector regulation | Planned |
| Reasoning Module API | Unified reasoning interface | Planned |
| Control Flow Graph (CFG) | Process-stage meta-context & tracing | Planned |
| Sleep / GC | Consolidation & pruning cycles | Partial |
| Safety Layer | Gating, risk & constraint enforcement | Planned |
| Audit / Logging | Trace, metrics, reproducibility | Partial |

*Status legend: Planned (design only), Partial (initial spec / not implemented), Implemented (code prototype exists).

# 3. [Detailed DMN Algorithm](./ideas/DMN.md)

- The DMN loop is the core recursive process for perception, memory, reasoning, and self-modeling.
- Operates in cycles: gathers sensory input, parses/dispatches tasks, generates and scores candidate thoughts, binds/expands context, explores and values thought graphs, selects actions, tags with reward, and consolidates memory.
- Supports mind-wandering, affect-driven savoring, and recursive re-entry; all modulated by neuromodulator and affect states.
- Key phases:
  - Input gathering & preprocessing
  - Structured parsing & dispatch
  - Iterative candidate generation & scoring
  - Associative memory expansion
  - Value-based exploration & selection
  - Reward tagging & memory consolidation
  - Narrative self-modeling & world model update
  - Mind-wandering & affect-driven routines
  - Continuous recursive re-entry

# Summary of Neuromodulator Impact on Algorithms

| Neuromodulator | Algorithmic Effects |
| --- | --- |
| Dopamine (DA) | Increases novelty weight w_DA, explorationÂ budget, consolidation priority, reward signaling; promotesÂ broad associative searchÂ ("panning"). |
| Serotonin (5HT) | Opens mind-wandering gate; raises safety penalty w_SAFE; favors positive/safe memory paths; decreases riskÂ appetite. |
| Norepinephrine (NE) | Controls beam search width and depthÂ (focus vs exploration); increases urgency and search depth; biases toward highlyÂ relevant/urgent memoriesÂ and thoughts. |
| Oxytocin (OXT) | Heightens prosocial prior w_SOC, boosts socialÂ memory recall and identity coherence weightÂ w_ID. |
| Testosterone (TST) | Increases assertive, goal-seeking weights; raises cost-delay penalties; counterbalanced by serotonin for risk management. |
| Orexin (ORX) | Primary wake-state maintenance; enables sustained goal-directed behavior and locomotor activity;Â modulates arousal threshold for external stimuli;Â amplifies histamine release and cognitive wakefulness; gatesÂ transition from sleep/consolidation backÂ to active DMN loop. |
| Histamine (HA) | Core wakefulness signal enabling cognitive processing and attention; maintains conscious awareness and prevents sleepÂ transitions; whenÂ depleted belowÂ threshold, triggersÂ sleep/consolidation mode; supports workingÂ memory and executiveÂ function duringÂ active cognition. |

# Module Details (Condensed)
(See individual specs in `ideas/` for full detail.)
- NeuroTransmitterEmitterModule: Simulated nuclei â†’ NT vector outputs.
- ProjectionModulator: Per-edge 1-layer modulators scaling NT transfer (homeostasisâ€‘trainable).
- Receptors: Binding & sensitivity; enforces local caps.
- AffectModulator: Adjust affect vector components under safety constraints.
- HomeostasisModule: Maintains NT bands, minimizes volatility vs performance loss.
- ReasoningModule: Standardized reasoning/query interface with neuromodulator context.
- VisionaryMemory: Prospective constructs & Expected Prospective Value (EPV).
- IdentityMemory: Entity/person modeling (traits, trust, alignment, relational edges).
- SelfModelingLayer: Predictive self-dynamics, drift forecasting, counterfactuals.
- Input/Language Optimizer: Pre-MDN normalization & uncertainty injection.
- CFG Meta Layer: Stage tags & provenance for process-aware prompting.

# Metrics Appendix
Formal definitions (t = tick index):
- Coherence Score (coherence_t): Fraction of active thought chain propositions entailed / non-contradictory w.r.t. narrative + goals.
  coherence_t = (|Supported| - |Contradicted|) / max(1, |Chain|)
- Drift Rate (drift_t): drift_t = || z_self_full_t - z_self_full_{t-Î”} || / Î” (L2 norm; Î” configurable window)
- Self Prediction Error (spe_t): spe_t = mean_f || ÅœelfState_t[f] - SelfState_t[f] || over monitored facets
- Calibration Loss (cal_t): Negative log-likelihood of observed error events under predicted uncertainty OR Brier across discretized error bins
- Expected Prospective Value (EPV): EPV(v) = Î£_k a_k Â· feature_k(v) with feature_k âˆˆ {utility, feasibility, -risk, identity_alignment, novelty, -constraint_violation, -effort_cost, -time_discount}
- Stability Penalty (stab_t): Weighted sum volatility_t^n across NT types; volatility_t^n = |Î¼_t^n - Î¼_{t-1}^n|
- Band Penalty (band_t): Î£_{areas, n} max(0, Î¼_t^n - upper_band^n, lower_band^n - Î¼_t^n)
- Identity Alignment (align_t(entity)): cosine( value_orientation âŠ• trait_vector, self_value_trait )
- Narrative Integrity (narr_int_t): 1 - contradiction_rate among active narrative threads vs new chain
- Exploration Index (explore_t): normalized proportion of novel candidates kept (similarity below Ï„) per tick
- Trust Calibration Error (tce_t): | predicted_trust_{iâ†’j} - realized_reliability_{iâ†’j} | averaged over relationships
- Counterfactual Advantage (cf_adv_t): value_actual_t - max(value_counterfactual_set_t)
- Reward Augmentation: R_total = R_task + Î²1Â·coherence_gain - Î²2Â·drift_t - Î²3Â·cal_t - Î²4Â·stab_t - Î²5Â·band_t + Î²6Â·cf_adv_t

## Reward & Stability Formalization
Symbols (all dimensionless unless stated):
- t: tick index (ms-scale loop; 1 tick â‰ˆ 50â€“200 ms simulated wall time)
- Î¼_t^n: neuromodulator n level (normalized [0,1])
- Î”Î¼_t^n = Î¼_t^n âˆ’ Î¼_{t-1}^n
- V_cand: value estimate of candidate chain (task utility units)

Stability Metrics:
- Volatility_t = Î£_n w_vol^n Â· |Î”Î¼_t^n|  (units: normalized NT change)
- BandViolation_t = Î£_{areas,n} max(0, Î¼_t^n âˆ’ U_n, L_n âˆ’ Î¼_t^n)  (U_n, L_n are upper/lower bands)
- Drift_t = || z_self_full_t âˆ’ z_self_full_{tâˆ’Î”} ||_2 / Î”  (Î” = window size in ticks)
- CoherenceGain_t = coherence_t âˆ’ coherence_{tâˆ’1}
- CalibrationLoss_t = âˆ’ (1/M) Î£_{m=1..M} log p_pred(error_m)  (cross-entropy over error events)
- EPV_path = Î£_k a_k Â· feature_k(path)  (features normalized to [âˆ’1,1])

Self-Model Augmented Reward:
R_self_t = Î±1Â·CoherenceGain_t âˆ’ Î±2Â·Drift_t âˆ’ Î±3Â·CalibrationLoss_t âˆ’ Î±4Â·Volatility_t âˆ’ Î±5Â·BandViolation_t + Î±6Â·CFAdv_t

Total Tick Reward:
R_total_t = R_task_t + R_self_t âˆ’ Î±7Â·SafetyPenalty_t âˆ’ Î±8Â·EnergyCost_t
EnergyCost_t = Î£_{areas,n} c_n Â· transferred_{t}^{n}
SafetyPenalty_t = Î£_h Î»_hÂ·1_{constraint_h violated}

Projection/Homeostasis Update Signal (per NT n):
Adv_nt_t^n = (âˆ’ w_vol^nÂ·|Î”Î¼_t^n| âˆ’ w_band^nÂ·BandViolation_t^n + w_perf^nÂ·PerfDelta_t)  (PerfDelta_t = task performance delta)
Edge modulator gradient scales with Adv_nt_t^n Â· âˆ‚s_ij^n/âˆ‚Î¸

Clarification: PerfDelta_t
Primary definition (normalized EWMA delta):
- Maintain EWMA of task reward RÌ„_task_t = (1âˆ’Î»_perf)Â·RÌ„_task_{tâˆ’1} + Î»_perfÂ·R_task_t (Î»_perf small).
- PerfDelta_t = (RÌ„_task_t âˆ’ RÌ„_task_{tâˆ’1}) / (|RÌ„_task_{tâˆ’1}| + Îµ_perf).
Fallback robust window form (for sparse rewards):
- Keep sliding window W_perf of past rewards; median m_t and MAD_t.
- PerfDelta_t_robust = (R_task_t âˆ’ m_t) / (MAD_t + Îµ_perf).
Use primary for dense / frequent reward, fallback for sparse regimes (auto-switch if reward_nonzero_rate < r_sparse_threshold, default 0.15).

# Learning Process Scheduling & Priority
Define priority weights P_module (higher = updated first when budget limited):
| Module | Trigger | Priority | Budget Heuristic |
|--------|---------|----------|------------------|
| Safety Constraints | Any hard constraint risk | 100 | Always pre-emptive |
| ILO Adapter | Input error rate spike > Î¸ | 70 | â‰¤ 2% tick time |
| SML Predictor P_self | spe_t > median(spe) + Ïƒ | 60 | â‰¤ 5% tick time |
| Drift Forecaster D_drift | drift_t z-score > Î¸ | 55 | batch only if flagged |
| Projection Modulators | Adv_nt variance > Î¸ | 50 | cap params updated <5% per tick |
| Homeostasis Critic | every N_h ticks | 45 | offline preferred |
| Candidate Scoring Head | advantage batch â‰¥ B_min | 40 | micro-batch |
| Identity Updates | new relational episode | 35 | incremental EMA |
| Visionary Value | EPV error > Î¸ | 30 | sleep refinement |
| Memory Graph Embeds | new edges > E_min | 25 | defer if latency high |

Scheduling Rule: Execute updates in descending priority until cumulative update_time â‰¤ Ï„_update_budget (fraction of tick, e.g., 25%). Defer lower priority modules to next tick or offline window.

## Workspace Capacity & Latency Constraints
Let:
- B_max: max tokens (or semantic units) in broadcast workspace (e.g., 2048)
- C_max: max candidate thoughts per generation cycle (e.g., 24)
- P_max: max paths explored in VS expansion (e.g., beam width 8 Ã— depth 3)
- A_max: attribution entries per facet (e.g., top-k=6)

Enforcement:
- Truncate enriched context to top salience-ranked tokens until â‰¤ B_max.
- Adaptive C_max': C_max' = floor(C_max Â· (1 âˆ’ latency_pressure_t)), where latency_pressure_t = clamp(latency_obs / latency_budget âˆ’ 1, 0, 0.5).
- If predicted thinking latency > Ï„_tick_budget, reduce beam depth before pruning candidates (keep safety checks intact).

Latency Model:
latency_pred_t = Î£_phase (op_count_phase / throughput_phase)
Reject new optional expansions if latency_pred_t > Ï„_tick_budget.

## Memory Growth Control
Each new node i receives a retention score R_i:
R_i = w_salÂ·salience_i + w_novÂ·novelty_i + w_taskÂ·task_relevance_i âˆ’ w_dupÂ·duplication_i âˆ’ w_costÂ·storage_cost_i
Retention probability p_ret(i) = Ïƒ(Î³ (R_i âˆ’ Î¸_ret))
If p_ret(i) < u (u ~ Uniform[0,1]) â†’ node cached for short-term only.

Deduplication:
If cosine(embedding_i, embedding_j) > Ï„_dup and |timestamp_i âˆ’ timestamp_j| < Î”_dup â†’ merge counts & edges.

Periodic Compression (Sleep):
- Cluster size > K_min & intra-cluster variance < v_max â†’ form abstract node; collapse members.
- Drop nodes with R_i < Î¸_prune and age > T_stale.
Target Complexity Budget:
- |E| (edges) maintained so that |E| â‰¤ Îº Â· |V| log |V|; prune lowest weight associative edges first.

## Safety & Constraint Framework
Constraint Classes:
- Hard (H): Must never be violated (e.g., Î¼ ceilings, forbidden content categories, max drift_risk).
- Soft (S): Penalized but violable (e.g., mild over-band, low coherence drop).

Formal Set:
H = { h_k(x_t) â‰¤ 0 }; if any h_k > 0 â†’ immediate mitigation action (abort candidate, clamp Î¼, invoke safety policy).
S = { s_j(x_t) â‰¤ 0 }; penalty term Î»_jÂ·max(0, s_j).

Enforcement Order (per tick):
1. Predictive Pre-Check: filter candidates with predicted hard violation probability > p_hard_max.
2. Runtime Gating: evaluate fast surrogates for H; if fail â†’ discard candidate / apply fallback plan.
3. Post-Selection Audit: verify chosen chain; if violation detected â†’ rollback and select next best safe chain.
4. Actuation Guard: final Î¼ clamps & rate limiters before external action.
5. Logging & Escalation: record violation attempts; raise adaptive tightening if frequency > f_thresh.

Constraint Solving (Optional Offline):
Formulate minimal modification Î”c to candidate chain c minimizing ||Î”c|| subject to H satisfied and total penalty Î£_j max(0, s_j(c+Î”c)) minimized (L1). Approximated via greedy repair (remove highest offending segment first) or small MILP for structured actions.

Verification Hooks:
- Unit tests for invariant preservation (Î¼ â‰¤ U, drift_risk â‰¤ Î¸_drift_hard).
- Statistical monitoring: P(hard_violation) estimated with Wilson interval; enforce upper bound.

## Documentation Drift Mitigation
- Single source of truth: this module inventory & appendix sections versioned; all other docs reference identifiers only.
- Lint script (planned) parses README module table vs `ideas/` filenames to flag divergence.

## Units & Normalization Summary
| Quantity | Range / Units | Normalization |
|----------|---------------|---------------|
| Î¼_t^n | [0,1] | band-pass scaled |
| Coherence Score | [âˆ’1,1] | symmetric (contradiction heavy negative) |
| Drift Rate | â‰¥0 | window-normalized |
| Calibration Loss | â‰¥0 | per event average |
| Volatility | â‰¥0 | weighted NT delta sum |
| EPV | â„ | features normalized, coefficients bounded |
| R_total | â„ | scaled by task reward magnitude |
| latency_pred_t | ms (simulated) | compared to Ï„_tick_budget |

(Sections above address previously noted gaps: explicit equations, scheduling priorities, capacity/latency constraints, memory growth control, and formal safety constraints.)

# Learning Schedule (Cadence)

| Component | Per Tick (Online) | Sleep / Offline (Batch) |
|-----------|-------------------|-------------------------|
| P_self (SML predictor) | Light SGD every N_pred | Full epochs, recalibrate Ïƒ |
| D_drift forecaster | Update on drift events | Re-train with balanced data |
| CF_self (counterfactual) | On top-k selections | Multi-step rollouts distill |
| Candidate Scoring Head | Advantage updates | Re-weight importance samples |
| Projection Modulators | Band error gradients | Smoothness & sparsity regs |
| Homeostasis Module | Actor step if Î”metrics | Critic fit & band re-center |
| Memory Graph Embeds | Incremental edge adds | Recompute / prune / compress |
| Visionary Value Models | On EPV deltas | Feasibility / risk refits |
| Identity Traits/Trust | EMA updates per episode | Drift audit & re-basing |
| Input Optimizer Adapter | Cache corrections | Fine-tune correction model |
| CFG Meta Logs | Append provenance | Aggregate analytics |
| Safety Policies | Threshold checks | Threshold auto-tuning |

Legend: N_pred: small fixed interval (e.g., every 2â€“3 ticks)


# Experimental Roadmap (Brief)
1. Minimal DMN + Memory Graph + ILO baseline.
2. Add neuromodulator scalars & Projection/Homeostasis modules; measure NT volatility vs performance.
3. Introduce SML; track drift and calibration reductions.
4. Layer Visionary & Identity; evaluate EPV retention & trust calibration.
5. Enable counterfactual self simulation; measure cf_adv_t utilization.

# Ethical Stance (Recap)
No phenomenological feeling simulation; avoids artificial suffering risk while enabling functional self-awareness.

# Changelog (Documentation Hygiene)
- Duplicated module listings removed.
- Added consolidated module inventory.
- Added SML & ILO summaries.
- Added Metrics Appendix & Learning Schedule diagram.

(Older duplicated module descriptions removed for clarity.)

## Empirical Optimization & Initialization (To Be Tuned Experimentally)
The following components require empirical estimation & iterative optimization; defaults below are starting anchors, not final values.

### Default Hyperparameter Seeds
| Symbol | Meaning | Default | Notes |
|--------|---------|---------|-------|
| Î±1..Î±6 | Self reward weights (coherence, drift, calib, volatility, band, cf_adv) | 0.8, 0.6, 0.4, 0.3, 0.3, 0.5 | Scale so |R_self| â‰ˆ |R_task|/2 initially |
| Î±7, Î±8 | Safety, energy penalties | 1.0, 0.2 | Safety dominant early |
| Î²1..Î²6 | Reward augmentation weights (legacy form) | mirror Î±1..Î±6 | If using combined form keep consistency |
| w_vol^n | Per NT volatility weight | all 1.0 | Later learned via regression vs performance loss |
| w_band^n | Band violation weights | all 0.8 | Raise for sensitive NTs (e.g., NE) |
| U_n / L_n | NT upper / lower bands | 0.75 / 0.25 | Adapt via band occupancy stats |
| Î¸_drift_hard | Hard drift risk threshold | 0.15 | Normalized drift units |
| Î¸_ret | Retention threshold | 0.0 | Centered after z-scoring features |
| Î³ (retention sharpness) | Logistic slope | 3.0 | Adjust for target acceptance rate ~60% |
| Ï„_dup | Duplication cosine threshold | 0.95 | Tune to trade off storage vs recall redundancy |
| K_min | Min cluster size for abstraction | 6 | Increase as memory grows |
| v_max | Max intra-cluster variance | 0.35 | In normalized embedding space |
| Îº | Edge complexity factor in |E| â‰¤ Îº |V| log|V|| | 2.0 | Tighten if latency grows |
| p_hard_max | Max predicted hard violation prob | 0.02 | Conservative gate |
| Î· | Adaptive weighting exponent | 0.25 | Slows oscillations |
| N_pred | SML light update interval (ticks) | 3 | Adjust by spe variance |
| B_max | Workspace tokens | 2048 | Hardware dependent |
| C_max | Candidate thoughts per cycle | 24 | Scales with latency budget |
| P_max | Path expansion (beam*depth) | 8*3 | Tune for value coverage |
| Î»_cal | Calibration variance EMA rate | 0.02 | Higher = faster adaptation, noisier |
| k_overconf | Overconfidence threshold multiplier k | 4.0 | Flag if Îµ^2 > kÂ·ÏƒÌ‚^2 |
| Î»_perf | EWMA performance smoothing rate | 0.05 | For PerfDelta_t primary form |
| W_perf | Reward window length (ticks) | 200 | For robust PerfDelta_t form |
| Îµ_perf | Small denom stabilizer | 1e-6 | Avoid divide-by-zero |
| H_EPV | EPV retention horizon (ticks) | 300 | ~15s if 20 Hz loop |
| Î±_prior, Î²_prior | Laplace smoothing counts | 1.0, 1.0 | For p_pred frequency blending |
| r_sparse_threshold | Sparse reward rate switch | 0.15 | Below â†’ use robust form |

### Calibration Update Procedure
Assume per-facet prediction errors Îµ_t,f ~ N(0, Ïƒ_f^2). Maintain rolling mean Î¼_f (â‰ˆ0) and variance v_f.
1. After each observation: v_f â† (1âˆ’Î»_cal)Â·v_f + Î»_calÂ·Îµ_t,f^2.
2. Predicted variance ÏƒÌ‚_f^2 is passed from model; calibration loss per facet: L_cal_f = 0.5Â·(log ÏƒÌ‚_f^2 + Îµ_t,f^2 / ÏƒÌ‚_f^2).
3. Variance shrinkage: enforce ÏƒÌ‚_f^2 â† max(Ïƒ_min^2, ÏƒÌ‚_f^2Â·(1 âˆ’ ÏÂ·overconfident_flag)), where overconfident_flag = 1 if Îµ_t,f^2 > k_overconfÂ·ÏƒÌ‚_f^2.
4. Reliability bins: bucket standardized errors z_f = Îµ/ÏƒÌ‚_f; maintain histogram; KL divergence between empirical bin distribution and N(0,1) used as auxiliary penalty.
5. Estimating p_pred(error_m):
   - Model outputs raw probability pÌ‚_model.
   - Maintain empirical bin frequencies with Laplace smoothing: pÌ‚_emp(bin) = (count_error_bin + Î±_prior)/(count_total_bin + Î±_prior + Î²_prior).
   - Blend: p_pred = (1âˆ’w_freq)Â·pÌ‚_model + w_freqÂ·pÌ‚_emp(bin), where w_freq increases toward 1 as bin count surpasses N_min (e.g., w_freq = 1 âˆ’ exp(âˆ’count_total_bin / N_min)). Default N_min = 50.
   - Optional recalibration: apply isotonic or Platt scaling offline using recent (pÌ‚_model, outcome) pairs if Brier score plateau > threshold.

### Adaptive Weighting of Reward Components
Maintain target metric bands (lower*, upper*). For metric m with instantaneous value m_t and target midpoint m*:
Î²_m â† Î²_m Â· (m*/ max(Îµ_m, m_t))^Î·  (Îµ_m small constant to avoid divide by zero).
Clamp Î²_m within [Î²_min, Î²_max] to prevent runaway (e.g., [0.05, 2.5]). Update every T_weight ticks (e.g., 20) to avoid high-frequency oscillations.

### Retention Feature Normalization
For features F âˆˆ {salience, novelty, task_relevance, duplication, storage_cost} maintain sliding window W (e.g., 500â€“2000 recent candidates):
- Compute z_F = (F âˆ’ mean_W(F)) / (std_W(F)+Îµ).
- duplication_i defined as max cosine similarity to retained nodes (higher = more duplicate) then z-scored and enters formula with negative sign.
- storage_cost_i can be proportional to raw token length / compression ratio.
Retention score R_i uses z-scored values ensuring dimensionless combination.

### Empirical Validation Protocol
A/B (or ablation) study stages:
1. Baseline: DMN + Memory Graph + ILO (no SML, no NT modulation) â†’ record metrics baseline_{coh, drift, spe, latency}.
2. Add Neuromodulation & Homeostasis: measure Î”coherence, Î”volatility variance reduction; success if volatility std â†“ â‰¥20% with â‰¤5% task performance loss.
3. Add SML: success if drift_t median â†“ â‰¥15% and spe_t â†“ â‰¥25% vs stage 2.
4. Add Visionary Memory: EPV retention (fraction of active goals still referenced after H_EPV horizon) â†‘ â‰¥15% vs stage 3. Define EPV retention metric: Ret_EPV = |goals_active_t âˆ© goals_active_{t+H_EPV}| / |goals_active_t| over valid non-empty sets.
5. Add Identity Memory: trust calibration error tce_t â†“ â‰¥20% vs stage 4.
6. Enable Counterfactual Self Simulation: cf_adv utilization (proportion ticks with positive advantage applied) â‰¥40%; coherence_t variance stable.
All tests logged with statistical significance (e.g., bootstrapped confidence intervals; require non-overlap at 95% for success). Revert module if regression > threshold.

### Stability Monitor (Lyapunov-like Proxy)
Define candidate Lyapunov function V_t = Drift_t + Î»_volÂ·Volatility_t + Î»_bandÂ·BandViolation_t.
Estimate expected change given action class a: E[Î”V | a] â‰ˆ (1/K) Î£_{k in recent actions of class a} (V_{k+1} âˆ’ V_k).
Action classes: {internal_reflection, memory_expansion, external_output, modulation_adjust, counterfactual_eval}.
Policy Constraint: Disallow sequences where E[Î”V | a] > Î¸_V for L consecutive proposals; trigger mitigation (insert stabilization action class) if rolling mean of V_t increases 3 consecutive windows.
Adaptive Î» coefficients tuned to keep V_t within target band (V_low, V_high); if V_t > V_high â†’ increase weight on stabilization candidates.

### Empirical Tracking Dashboard (Suggested)
- Rolling Î²_m values vs target metrics.
- Calibration reliability plot (z histogram vs N(0,1)).
- Retention acceptance rate vs target (60%).
- V_t trajectory with flagged excursions.
- Ablation stage comparison table (metric deltas & CI).

(Section provides explicit starting defaults and empirical optimization plan to reduce ambiguity and support reproducible experimentation.)

## [Support & Sponsorship](./SPONSOR.md)
We are seeking collaborators and sponsors to accelerate empirical validation of this blueprint.

Why Funding Matters:
- Embodied Simulation: Running multi-agent / sensory scenarios in NVIDIA Isaac Sim (cloud) to generate rich episodic data and stress-test memory consolidation & self-modelling under realistic sensorimotor loops.
- GPU Experimentation: Sustained access to mid/high-tier GPUs (A100/L40/3090 class) for training SML predictors, counterfactual rollouts, and embedding recalibration beyond free-tier constraints (Google Colab / Kaggle limits are insufficient for continuous loops).
- Ablation Throughput: Parallel seeded runs (â‰¥30 seeds per configuration) to produce statistically robust confidence intervals for drift, coherence, volatility and calibration metrics.
- Tooling & Instrumentation: Development of reliability calibration dashboards, stability monitor visualizers, and automated ablation harness.

Immediate Funding Targets (Indicative Monthly Burn):
- Cloud Isaac Sim cluster (containers + RTX GPU hours): $600â€“$1,200
- GPU training time (on-demand or reserved instances): $800â€“$1,500
- Storage & Logging (object storage + telemetry DB): $80â€“$150
- CI / Automation (compute minutes + artifact retention): $50â€“$120
Total (lean baseline): â‰ˆ $1.5kâ€“$3.0k / month

Use of Funds (Prioritized):
1. Reproducible Simulation Scenarios (sensorimotor task suite + scripted perturbations)
2. Stability & Calibration Experiment Runs (multi-seed batches)
3. Scaling Counterfactual Rollouts (multi-step CF_self efficiency profiling)
4. Memory Retention / Compression Experiments (long-horizon runs > 100k ticks)
5. Open Dashboards & Reporting (public metric artifacts, notebooks)

How to Support:
- Direct Sponsorship / Grants: Contact (add email or form) for contribution agreements.
- GPU Credits / Cloud Vouchers: Provide cloud credits earmarked for logged experiment IDs.
- Hardware Donation: Provision remote GPU workstation access (documented + sandboxed environment).
- Research Collaboration: University / lab partnership for shared publication pipeline.
- Open Source Contributions: Implement phases from [ROADMAP.md](./ROADMAP.md) reducing engineering overhead.

Transparency & Accountability:
- Monthly public expenditure + utilization summary (GPU hours, sim hours, runs completed).
- Open metric artifacts (hash-addressed) for every funded experiment.
- Sponsorship acknowledgment in README + papers (unless anonymity requested).
- Explicit ethical guardrails: no phenomenological suffering simulation; strict safety constraint instrumentation.

Milestone Funding Tranches (Suggested):
- Milestone 1 (Baseline Loop + Metrics Harness Complete): 100% functional Phase 0â€“5.
- Milestone 2 (SML + Drift/Stability Operational): Phases 6â€“8 validated.
- Milestone 3 (Visionary + Identity + Counterfactual v1): Phases 9â€“11.
- Milestone 4 (Adaptive Stability + Retention + Calibration): Phases 12â€“15.
- Milestone 5 (Full Monitor, Multi-Step CF, Ablation Harness): Phases 16â€“19.

If you or your organization can help, open an issue titled â€œSponsorship Offerâ€ or reach out directly. Strategic support now can accelerate an empirically grounded step toward functional artificial self-modelling.

(Interested sponsors: please avoid imposing proprietary licensing constraints; maintaining openness is central to scientific impact.)


